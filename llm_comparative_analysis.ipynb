{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shalvigour/LLM_COMPARATOR/blob/main/llm_comparative_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kWzqqdaNGag"
      },
      "source": [
        "# Comparative Analysis of Large Language Model (LLM) Platforms\n",
        "\n",
        "This Jupyter Notebook provides a framework for comparing various commonly used Large Language Model (LLM) platforms. The goal is to help you understand the differences in their performance (latency, token usage), ease of use, and model availability for your specific applications.\n",
        "\n",
        "We will explore the following platforms:\n",
        "\n",
        "1.  **OpenAI**: Known for state-of-the-art models like GPT-4o, GPT-4, and GPT-3.5.\n",
        "2.  **Hugging Face Inference API**: A hub for open-source LLMs and tools, offering flexibility to use a vast array of models.\n",
        "3.  **Groq**: Specializes in high-speed, low-latency inference for certain LLMs.\n",
        "4.  **Ollama**: For running open-source LLMs locally, providing privacy and cost control.\n",
        "5.  **Google Gemini (via Google AI Studio/Vertex AI)**: Google's multimodal LLM family.\n",
        "6.  **Anthropic (Claude)**: Known for its focus on safety and constitutional AI.\n",
        "7.  **Mistral AI**: Popular for its powerful and efficient open-source models, often with competitive performance.\n",
        "\n",
        "## Key Aspects for Comparative Analysis\n",
        "\n",
        "When comparing these platforms, we will primarily focus on:\n",
        "\n",
        "* **Performance (Speed & Quality)**:\n",
        "    * **Latency**: How quickly does the model respond (Time to First Token & Time to Last Token)?\n",
        "    * **Token Usage**: Number of input and output tokens, which directly impacts cost.\n",
        "    * **Qualitative Output**: The quality, coherence, and relevance of the generated response (requires manual review).\n",
        "* **Cost**: While not directly calculated in code, token usage data will inform cost estimations (refer to official pricing pages).\n",
        "* **Ease of Use & Integration**: API simplicity and library support.\n",
        "* **Model Availability & Flexibility**: Proprietary vs. open-source models, fine-tuning capabilities.\n",
        "\n",
        "**Disclaimer**: Latency measurements can be affected by network conditions, API server load, and the specific model chosen. For robust benchmarks, multiple runs and averaging are recommended."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSQO1EZwNGak"
      },
      "source": [
        "## 1. Setup and Environment Configuration\n",
        "\n",
        "First, we need to install all the necessary Python libraries for interacting with each LLM platform. We will also set up environment variables for API keys to ensure security."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPx79N1nNGak",
        "outputId": "cbb5a077-ca18-4991-ea50-91d4b3f6b4ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.86.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.11/dist-packages (0.54.0)\n",
            "Requirement already satisfied: mistralai in /usr/local/lib/python3.11/dist-packages (1.8.2)\n",
            "Requirement already satisfied: ollama in /usr/local/lib/python3.11/dist-packages (0.5.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.3)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.172.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.4.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install openai huggingface_hub groq google-generativeai anthropic mistralai ollama python-dotenv\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata # Import userdata\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "# Create a file named `.env` in the same directory as this notebook.\n",
        "# Add your API keys to the .env file like this:\n",
        "# OPENAI_API_KEY=\"your_openai_api_key_here\"\n",
        "# HF_TOKEN=\"your_huggingface_token_here\"\n",
        "# GROQ_API_KEY=\"your_groq_api_key_here\"\n",
        "# GOOGLE_API_KEY=\"your_google_api_key_here\"\n",
        "# ANTHROPIC_API_KEY=\"your_anthropic_api_key_here\"\n",
        "# MISTRAL_API_KEY=\"your_mistral_api_key_here\"\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ADjrsfkNGal"
      },
      "source": [
        "### Standardized Prompt for Comparison\n",
        "\n",
        "To ensure a fair comparison, we will use the exact same prompt for all LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpMsmLeINGam"
      },
      "outputs": [],
      "source": [
        "test_prompt = \"Explain the concept of Artificial General Intelligence (AGI) in one paragraph, focusing on its potential impact on society and daily life.\"\n",
        "MAX_TOKENS = 150 # Max tokens for generated response (to keep output length somewhat consistent)\n",
        "TEMPERATURE = 0.7 # Creativity/randomness of the response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2LhU91eNGam"
      },
      "source": [
        "## 2. LLM Platform Implementations\n",
        "\n",
        "Below, we provide Python functions to interact with each LLM platform. Each function will attempt to query the model, measure latency, and extract token usage information where available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUB_jfLyNGam"
      },
      "source": [
        "### 2.1. OpenAI\n",
        "\n",
        "**Description**: OpenAI offers a suite of powerful models, including the widely popular GPT series. It's known for its strong performance across various tasks.\n",
        "\n",
        "**API Key Required**: Yes (`OPENAI_API_KEY`). Obtain from [platform.openai.com](https://platform.openai.com/).\n",
        "\n",
        "**Installation**: `pip install openai` (already included in the initial setup)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjq7zQqlNGan",
        "outputId": "6ef6037f-06b7-4920-b512-fc46d6b811ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- OpenAI Test ---\n",
            "Response: Artificial General Intelligence (AGI) refers to a type of artificial intelligence that possesses the ability to understand, learn, and apply knowledge across a wide range of tasks at a level comparabl...\n",
            "Latency: 4.3404 seconds\n",
            "Tokens: {'prompt_tokens': 33, 'completion_tokens': 150, 'total_tokens': 183}\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "def query_openai(prompt: str, model: str = \"gpt-4o-mini\"): # You can try \"gpt-4o\" if you have access\n",
        "    \"\"\"Queries OpenAI's chat completion API.\"\"\"\n",
        "    api_key = userdata.get('OPENAI_API_KEY')\n",
        "    if not api_key:\n",
        "        return \"API Key Missing: Please set OPENAI_API_KEY environment variable.\", None, None\n",
        "\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=MAX_TOKENS,\n",
        "            temperature=TEMPERATURE,\n",
        "        )\n",
        "        latency = time.time() - start_time\n",
        "        content = response.choices[0].message.content\n",
        "        tokens_info = {\n",
        "            \"prompt_tokens\": response.usage.prompt_tokens,\n",
        "            \"completion_tokens\": response.usage.completion_tokens,\n",
        "            \"total_tokens\": response.usage.total_tokens\n",
        "        }\n",
        "        return content, latency, tokens_info\n",
        "    except Exception as e:\n",
        "        return f\"Error querying OpenAI: {e}\", None, None\n",
        "\n",
        "# Example Usage (uncomment to run individually)\n",
        "print(\"\\n--- OpenAI Test ---\")\n",
        "openai_response, openai_latency, openai_tokens = query_openai(test_prompt)\n",
        "if openai_response and \"Error:\" not in openai_response:\n",
        "    print(f\"Response: {openai_response[:200]}...\")\n",
        "    print(f\"Latency: {openai_latency:.4f} seconds\")\n",
        "    print(f\"Tokens: {openai_tokens}\")\n",
        "else:\n",
        "    print(openai_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5e-DbxBNGan"
      },
      "source": [
        "### 2.2. Hugging Face Inference API\n",
        "\n",
        "**Description**: Hugging Face is an open-source AI community providing a vast hub of pre-trained models. Their Inference API allows you to use many of these models without local setup.\n",
        "\n",
        "**API Key Required**: Yes (`HF_TOKEN`). Obtain from [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens).\n",
        "\n",
        "**Installation**: `pip install huggingface_hub` (already included)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0j8X2PbMNGan",
        "outputId": "fa0a2c5d-9b87-4351-fd9b-80b4453b6804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Hugging Face Test (Zephyr) ---\n",
            "Response:  Use clear and concise language, avoiding technical jargon where possible. Provide examples to illustrate your points....\n",
            "Latency: 1.5059 seconds\n",
            "Tokens: {'info': 'Token counts not directly available via simple text_generation, estimate based on length.'}\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "from google.colab import userdata # Import userdata\n",
        "\n",
        "def query_huggingface(prompt: str, repo_id: str = \"HuggingFaceH4/zephyr-7b-beta\"): # Or another suitable text generation model\n",
        "    \"\"\"Queries a model via Hugging Face Inference API.\"\"\"\n",
        "    # hf_token = os.getenv(\"HF_TOKEN\") # Comment out or remove this line\n",
        "    hf_token = userdata.get('HF_TOKEN') # Read from Colab secrets\n",
        "    if not hf_token:\n",
        "        return \"API Token Missing: Please set HF_TOKEN in Colab secrets.\", None, None\n",
        "\n",
        "    # Note: For simple text_generation, token counts might not be directly returned in the response object\n",
        "    # as consistently as with other providers. For precise token counts, you might need to use a tokenizer\n",
        "    # on the input/output text or explore `chat_completion` endpoints if the model supports it and returns usage.\n",
        "    client = InferenceClient(model=repo_id, token=hf_token)\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        response = client.text_generation(prompt=prompt, max_new_tokens=MAX_TOKENS, temperature=TEMPERATURE)\n",
        "        latency = time.time() - start_time\n",
        "        tokens_info = {\"info\": \"Token counts not directly available via simple text_generation, estimate based on length.\"}\n",
        "        return response, latency, tokens_info\n",
        "    except Exception as e:\n",
        "        return f\"Error querying Hugging Face: {e}\", None, None\n",
        "\n",
        "# Example Usage (uncomment to run individually)\n",
        "print(\"\\n--- Hugging Face Test (Zephyr) ---\")\n",
        "hf_response, hf_latency, hf_tokens = query_huggingface(test_prompt)\n",
        "if hf_response and \"Error:\" not in hf_response:\n",
        "    print(f\"Response: {hf_response[:200]}...\")\n",
        "    if hf_latency is not None: # Add check for None\n",
        "        print(f\"Latency: {hf_latency:.4f} seconds\")\n",
        "    if hf_tokens is not None: # Add check for None\n",
        "        print(f\"Tokens: {hf_tokens}\")\n",
        "else:\n",
        "    print(hf_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QhHwJmxNGao"
      },
      "source": [
        "### 2.3. Groq\n",
        "\n",
        "**Description**: Groq is a hardware company that has developed a Language Processing Unit (LPU) designed for extremely fast LLM inference. It offers several open-source models with very low latency.\n",
        "\n",
        "**API Key Required**: Yes (`GROQ_API_KEY`). Obtain from [console.groq.com](https://console.groq.com/).\n",
        "\n",
        "**Installation**: `pip install groq` (already included)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W23MhOHNGao",
        "outputId": "a0f3ac95-5d24-45ba-a1aa-bd235c00658e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Groq Test (Llama 3 8B) ---\n",
            "Response: Artificial General Intelligence (AGI) refers to a hypothetical AI system that possesses the ability to understand, learn, and apply knowledge across a wide range of tasks, much like human intelligence...\n",
            "Latency: 2.3682 seconds\n",
            "Tokens: {'prompt_tokens': 37, 'completion_tokens': 150, 'total_tokens': 187}\n"
          ]
        }
      ],
      "source": [
        "from groq import Groq\n",
        "from google.colab import userdata # Import userdata\n",
        "\n",
        "def query_groq(prompt: str, model: str = \"llama3-8b-8192\"): # Or \"mixtral-8x7b-32768\"\n",
        "    \"\"\"Queries Groq's API.\"\"\"\n",
        "    # api_key = os.getenv(\"GROQ_API_KEY\") # Comment out or remove this line\n",
        "    api_key = userdata.get('GROQ_API_KEY') # Read from Colab secrets\n",
        "    if not api_key:\n",
        "        return \"API Key Missing: Please set GROQ_API_KEY in Colab secrets.\", None, None\n",
        "\n",
        "    client = Groq(api_key=api_key)\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            model=model,\n",
        "            max_tokens=MAX_TOKENS,\n",
        "            temperature=TEMPERATURE,\n",
        "        )\n",
        "        latency = time.time() - start_time\n",
        "        tokens_info = {\n",
        "            \"prompt_tokens\": chat_completion.usage.prompt_tokens,\n",
        "            \"completion_tokens\": chat_completion.usage.completion_tokens,\n",
        "            \"total_tokens\": chat_completion.usage.total_tokens\n",
        "        }\n",
        "        return chat_completion.choices[0].message.content, latency, tokens_info\n",
        "    except Exception as e:\n",
        "        return f\"Error querying Groq: {e}\", None, None\n",
        "\n",
        "# Example Usage (uncomment to run individually)\n",
        "print(\"\\n--- Groq Test (Llama 3 8B) ---\")\n",
        "groq_response, groq_latency, groq_tokens = query_groq(test_prompt)\n",
        "if groq_response and \"Error:\" not in groq_response:\n",
        "    print(f\"Response: {groq_response[:200]}...\")\n",
        "    print(f\"Latency: {groq_latency:.4f} seconds\")\n",
        "    print(f\"Tokens: {groq_tokens}\")\n",
        "else:\n",
        "    print(groq_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrhFFHiZNGao"
      },
      "source": [
        "### 2.4. Ollama (Local)\n",
        "\n",
        "**Description**: Ollama allows you to run large language models locally on your own machine. This is great for privacy, cost control (after initial hardware investment), and development without relying on external APIs.\n",
        "\n",
        "**API Key Required**: No, but requires a local Ollama server running.\n",
        "\n",
        "**Setup Steps**:\n",
        "1.  **Download and Install Ollama**: Visit [ollama.com](https://ollama.com/) and follow the instructions for your OS.\n",
        "2.  **Pull a Model**: Open your terminal and run, for example, `ollama pull llama3` or `ollama pull mistral`.\n",
        "3.  **Ensure Ollama Server is Running**: Typically, Ollama starts automatically in the background after installation, but you can explicitly run `ollama serve` in your terminal if you encounter connection issues.\n",
        "\n",
        "**Installation**: `pip install ollama` (already included)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "cQddQmSMNGao",
        "outputId": "c9e24ab8-8ff4-4d26-bd9a-73ae7d4bdb7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Ollama Test (Llama 3) ---\n",
            "Response: Error querying Ollama: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download. Make sure Ollama server is running and model 'llama3' i...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported format string passed to NoneType.__format__",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-1964492623>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mollama_response\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"Error:\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mollama_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Response: {ollama_response[:200]}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Latency: {ollama_latency:.4f} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Tokens: {ollama_tokens}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to NoneType.__format__"
          ]
        }
      ],
      "source": [
        "import ollama\n",
        "\n",
        "def query_ollama(prompt: str, model: str = \"llama3\"): # Ensure this model is pulled locally\n",
        "    \"\"\"Queries a local Ollama model.\"\"\"\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        response = ollama.chat(\n",
        "            model=model,\n",
        "            messages=[{'role': 'user', 'content': prompt}],\n",
        "            options={'num_predict': MAX_TOKENS, 'temperature': TEMPERATURE} # Ollama options for max_tokens, temperature\n",
        "        )\n",
        "        latency = time.time() - start_time\n",
        "        content = response['message']['content']\n",
        "\n",
        "        # Ollama's Python library response for chat currently doesn't expose\n",
        "        # precise token usage directly in the same way as OpenAI/Groq for `ollama.chat()`.\n",
        "        # You might need to use `ollama.generate()` or estimate based on text length for cost analysis.\n",
        "        tokens_info = {\"info\": \"Token counts not directly available via ollama.chat(). Estimate based on text length or use /api/generate for details.\"}\n",
        "        return content, latency, tokens_info\n",
        "    except Exception as e:\n",
        "        return f\"Error querying Ollama: {e}. Make sure Ollama server is running and model '{model}' is pulled (e.g., `ollama pull {model}`).\", None, None\n",
        "\n",
        "# Example Usage (uncomment to run individually)\n",
        "print(\"\\n--- Ollama Test (Llama 3) ---\")\n",
        "ollama_response, ollama_latency, ollama_tokens = query_ollama(test_prompt)\n",
        "if ollama_response and \"Error:\" not in ollama_response:\n",
        "    print(f\"Response: {ollama_response[:200]}...\")\n",
        "    print(f\"Latency: {ollama_latency:.4f} seconds\")\n",
        "    print(f\"Tokens: {ollama_tokens}\")\n",
        "else:\n",
        "    print(ollama_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r151M0gNGap"
      },
      "source": [
        "### 2.5. Google Gemini (via Google AI Studio / Vertex AI)\n",
        "\n",
        "**Description**: Google Gemini is Google's family of multimodal LLMs, offering powerful capabilities for text, image, and other data types.\n",
        "\n",
        "**API Key Required**: Yes (`GOOGLE_API_KEY`). Obtain from [ai.google.dev/gemini-api/](https://ai.google.dev/gemini-api/).\n",
        "\n",
        "**Installation**: `pip install google-generativeai` (already included)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "TBdBOhawNGap",
        "outputId": "8fc5bd4c-47f9-42c2-b4df-85b5b6c2ba73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Google Gemini Test (gemini-2.5-flash) ---\n",
            "Response: Artificial General Intelligence (AGI) represents a hypothetical level of AI with human-level cognitive abilities, capable of understanding, learning, and applying knowledge across a wide range of task...\n",
            "Latency: 1.9239 seconds\n",
            "Tokens: {'prompt_token_count': 25, 'candidates_token_count': 114, 'total_token_count': 139}\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata # Import userdata\n",
        "import os # Keep os for consistency with other parts of the notebook\n",
        "import time # Ensure time is imported for latency calculation\n",
        "\n",
        "# Assume MAX_TOKENS and TEMPERATURE are defined globally or passed as arguments\n",
        "# For this example, let's define them if not already:\n",
        "MAX_TOKENS = 1000\n",
        "TEMPERATURE = 0.7\n",
        "test_prompt = \"Explain the concept of Artificial General Intelligence (AGI) in one paragraph, focusing on its potential impact on society and daily life.\"\n",
        "\n",
        "\n",
        "def query_google_gemini(prompt: str, model_name: str = \"gemini-2.0-flash-lite\"):\n",
        "    \"\"\"Queries Google Gemini via Google AI Studio/Vertex AI.\"\"\"\n",
        "    api_key = userdata.get('GOOGLE_API_KEY') # Read from Colab secrets\n",
        "    if not api_key:\n",
        "        return \"API Key Missing: Please set GOOGLE_API_KEY in Colab secrets.\", None, None\n",
        "\n",
        "    genai.configure(api_key=api_key)\n",
        "    model = genai.GenerativeModel(model_name)\n",
        "\n",
        "    generation_config = {\n",
        "        \"max_output_tokens\": MAX_TOKENS,\n",
        "        \"temperature\": TEMPERATURE,\n",
        "    }\n",
        "\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        response = model.generate_content(prompt, generation_config=generation_config)\n",
        "        latency = time.time() - start_time\n",
        "\n",
        "        content = \"N/A - No response generated.\"\n",
        "        tokens_info = {}\n",
        "\n",
        "        if response.candidates:\n",
        "            # Get the finish reason from the first candidate\n",
        "            finish_reason = response.candidates[0].finish_reason.name if response.candidates[0].finish_reason else \"UNKNOWN\"\n",
        "\n",
        "            if finish_reason == \"STOP\":\n",
        "                # Model completed successfully\n",
        "                content = response.text\n",
        "                if hasattr(response, 'usage_metadata'):\n",
        "                    tokens_info = {\n",
        "                        \"prompt_token_count\": response.usage_metadata.prompt_token_count,\n",
        "                        \"candidates_token_count\": response.usage_metadata.candidates_token_count,\n",
        "                        \"total_token_count\": response.usage_metadata.total_token_count\n",
        "                    }\n",
        "                else:\n",
        "                    tokens_info = {\"info\": \"Token usage_metadata not available in response (STOP).\"}\n",
        "            else:\n",
        "                # Model stopped due to a reason other than normal completion\n",
        "                content = f\"Generation stopped early. Finish reason: {finish_reason}. \"\n",
        "                if response.candidates[0].safety_ratings:\n",
        "                    safety_issues = [\n",
        "                        f\"{sr.category.name}: {sr.probability.name}\"\n",
        "                        for sr in response.candidates[0].safety_ratings\n",
        "                        if sr.probability.name != 'NEGLIGIBLE' and sr.probability.name != 'LOW' # Filter out low/negligible\n",
        "                    ]\n",
        "                    if safety_issues:\n",
        "                        content += f\"Safety issues detected: {'; '.join(safety_issues)}.\"\n",
        "                    else:\n",
        "                        content += \"No specific safety issues indicated despite early stop.\"\n",
        "                else:\n",
        "                    content += \"No safety ratings available.\"\n",
        "\n",
        "                # Still try to get token info if available, even for early stops\n",
        "                if hasattr(response, 'usage_metadata'):\n",
        "                    tokens_info = {\n",
        "                        \"prompt_token_count\": response.usage_metadata.prompt_token_count,\n",
        "                        \"candidates_token_count\": response.usage_metadata.candidates_token_count,\n",
        "                        \"total_token_count\": response.usage_metadata.total_token_count\n",
        "                    }\n",
        "                else:\n",
        "                    tokens_info = {\"info\": \"Token usage_metadata not available in response (EARLY_STOP).\"}\n",
        "\n",
        "        else:\n",
        "            # No candidates were generated at all (e.g., prompt blocked by safety)\n",
        "            content = \"No candidates generated. Prompt or response likely blocked.\"\n",
        "            if hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason:\n",
        "                content += f\" Prompt blocked due to: {response.prompt_feedback.block_reason.name}.\"\n",
        "            tokens_info = {\"info\": \"No candidates, check prompt_feedback.\"}\n",
        "\n",
        "        return content, latency, tokens_info\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error querying Google Gemini: {e}\", None, None\n",
        "\n",
        "# Example Usage\n",
        "print(\"\\n--- Google Gemini Test (gemini-2.5-flash) ---\")\n",
        "gemini_response, gemini_latency, gemini_tokens = query_google_gemini(test_prompt)\n",
        "\n",
        "# Improved printing to handle None values gracefully\n",
        "if \"Error:\" not in gemini_response:\n",
        "    print(f\"Response: {gemini_response[:200]}...\")\n",
        "    print(f\"Latency: {gemini_latency:.4f} seconds\" if gemini_latency is not None else \"Latency: N/A\")\n",
        "    print(f\"Tokens: {gemini_tokens}\")\n",
        "else:\n",
        "    print(gemini_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtgW2AZTNGap"
      },
      "source": [
        "### 2.6. Anthropic (Claude)\n",
        "\n",
        "**Description**: Anthropic's Claude models are known for their safety, helpfulness, and longer context windows, often preferred for tasks requiring careful reasoning.\n",
        "\n",
        "**API Key Required**: Yes (`ANTHROPIC_API_KEY`). Obtain from [console.anthropic.com/settings/keys](https://console.anthropic.com/settings/keys).\n",
        "\n",
        "**Installation**: `pip install anthropic` (already included)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "OJwPpoyRNGap",
        "outputId": "59fb18ca-9806-42f2-9ef5-92274f954aee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Anthropic Claude Test (Opus) ---\n",
            "Response: Error querying Anthropic Claude: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Pla...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported format string passed to NoneType.__format__",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-3150413258>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mclaude_response\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"Error:\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclaude_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Response: {claude_response[:200]}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Latency: {claude_latency:.4f} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Tokens: {claude_tokens}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to NoneType.__format__"
          ]
        }
      ],
      "source": [
        "import anthropic\n",
        "from google.colab import userdata # Import userdata\n",
        "\n",
        "def query_anthropic_claude(prompt: str, model: str = \"claude-sonnet-4-20250514\"): # Or \"claude-3-sonnet-20240229\", \"claude-3-haiku-20240307\"\n",
        "    \"\"\"Queries Anthropic's Claude API.\"\"\"\n",
        "    # api_key = os.getenv(\"ANTHROPIC_API_KEY\") # Comment out or remove this line\n",
        "    api_key = userdata.get('ANTHROPIC_API_KEY') # Read from Colab secrets\n",
        "    if not api_key:\n",
        "        return \"API Key Missing: Please set ANTHROPIC_API_KEY in Colab secrets.\", None, None\n",
        "\n",
        "    client = anthropic.Anthropic(api_key=api_key)\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        message = client.messages.create(\n",
        "            model=model,\n",
        "            max_tokens=MAX_TOKENS,\n",
        "            temperature=TEMPERATURE,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        latency = time.time() - start_time\n",
        "        tokens_info = {\n",
        "            \"input_tokens\": message.usage.input_tokens,\n",
        "            \"output_tokens\": message.usage.output_tokens,\n",
        "            \"total_tokens\": message.usage.input_tokens + message.usage.output_tokens\n",
        "        }\n",
        "        return message.content[0].text, latency, tokens_info\n",
        "    except Exception as e:\n",
        "        return f\"Error querying Anthropic Claude: {e}\", None, None\n",
        "\n",
        "# Example Usage (uncomment to run individually)\n",
        "print(\"\\n--- Anthropic Claude Test (Opus) ---\")\n",
        "claude_response, claude_latency, claude_tokens = query_anthropic_claude(test_prompt)\n",
        "if claude_response and \"Error:\" not in claude_response:\n",
        "    print(f\"Response: {claude_response[:200]}...\")\n",
        "    print(f\"Latency: {claude_latency:.4f} seconds\")\n",
        "    print(f\"Tokens: {claude_tokens}\")\n",
        "else:\n",
        "    print(claude_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omGRt-KyNGap"
      },
      "source": [
        "### 2.7. Mistral AI\n",
        "\n",
        "**Description**: Mistral AI is a French AI company gaining popularity for its powerful and efficient open-source models (like Mixtral) and competitive proprietary models (Mistral Small, Large).\n",
        "\n",
        "**API Key Required**: Yes (`MISTRAL_API_KEY`). Obtain from [console.mistral.ai](https://console.mistral.ai/).\n",
        "\n",
        "**Installation**: `pip install mistralai` (already included)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLr69pk3NGap",
        "outputId": "f2057cfe-391f-4614-8fc1-06e0cc8b529e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Mistral AI Test (Mistral Large) ---\n",
            "Response: Artificial General Intelligence (AGI) refers to the hypothetical ability of an intelligent agent to understand, learn, and apply knowledge across a wide range of tasks at a level equal to or even surp...\n",
            "Latency: 11.4806 seconds\n",
            "Tokens: {'prompt_tokens': 31, 'completion_tokens': 150, 'total_tokens': 181}\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "# Keep the import as per your working code\n",
        "from mistralai import Mistral\n",
        "# Remove the problematic import: from mistralai.models.chat import ChatMessage\n",
        "from google.colab import userdata # Import userdata\n",
        "import os # Keep os for consistency if you also use .env locally\n",
        "\n",
        "# Assuming MAX_TOKENS, TEMPERATURE, and test_prompt are defined globally or elsewhere\n",
        "# For this snippet, let's include them for completeness if not already present:\n",
        "MAX_TOKENS = 150\n",
        "TEMPERATURE = 0.7\n",
        "test_prompt = \"Explain the concept of Artificial General Intelligence (AGI) in one paragraph, focusing on its potential impact on society and daily life.\"\n",
        "\n",
        "\n",
        "def query_mistral(prompt: str, model: str = \"mistral-large-latest\"):\n",
        "    \"\"\"Queries Mistral AI's API.\"\"\"\n",
        "    api_key = userdata.get('MISTRAL_API_KEY') # Read from Colab secrets\n",
        "    if not api_key:\n",
        "        return \"API Key Missing: Please set MISTRAL_API_KEY in Colab secrets.\", None, None\n",
        "\n",
        "    # Initialize the client using the exact syntax from your working code\n",
        "    client = Mistral(api_key=api_key)\n",
        "\n",
        "    # Define messages using a dictionary directly, as per your working code\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        # Use the 'complete' method as per your working code\n",
        "        chat_response = client.chat.complete(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            max_tokens=MAX_TOKENS,\n",
        "            temperature=TEMPERATURE,\n",
        "        )\n",
        "        latency = time.time() - start_time\n",
        "\n",
        "        # Token usage information should still be available under chat_response.usage\n",
        "        tokens_info = {\n",
        "            \"prompt_tokens\": chat_response.usage.prompt_tokens,\n",
        "            \"completion_tokens\": chat_response.usage.completion_tokens,\n",
        "            \"total_tokens\": chat_response.usage.total_tokens\n",
        "        }\n",
        "        return chat_response.choices[0].message.content, latency, tokens_info\n",
        "    except Exception as e:\n",
        "        return f\"Error querying Mistral AI: {e}\", None, None\n",
        "\n",
        "# Example Usage\n",
        "print(\"\\n--- Mistral AI Test (Mistral Large) ---\")\n",
        "mistral_response, mistral_latency, mistral_tokens = query_mistral(test_prompt)\n",
        "\n",
        "# Robust printing to handle None values gracefully\n",
        "if mistral_response and \"Error:\" not in mistral_response:\n",
        "    print(f\"Response: {mistral_response[:200]}...\")\n",
        "    print(f\"Latency: {mistral_latency:.4f} seconds\" if mistral_latency is not None else \"Latency: N/A\")\n",
        "    print(f\"Tokens: {mistral_tokens}\")\n",
        "else:\n",
        "    print(mistral_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXCzRUz7NGaq"
      },
      "source": [
        "## 3. Aggregated Comparative Analysis\n",
        "\n",
        "Now, let's run the standardized prompt across all platforms and gather the results in a structured format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "MgBI6zCrNGaq",
        "outputId": "ad62d9cf-afd0-47f5-e416-df8cac498519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Comparative Queries ---\n",
            "Querying OpenAI (gpt-4o-mini)...\n",
            "Querying Hugging Face (Zephyr-7b-beta)...\n",
            "Querying Groq (Llama 3 8B)...\n",
            "Querying Ollama (llama3) - ensure server is running locally...\n",
            "Querying Google Gemini (gemini-1.5-flash-latest)...\n",
            "Querying Anthropic Claude (claude-3-haiku-20240307)...\n",
            "Querying Mistral AI (mistral-small-latest)...\n",
            "--- Comparative Queries Complete ---\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "\n",
        "print(\"--- Starting Comparative Queries ---\")\n",
        "\n",
        "print(\"Querying OpenAI (gpt-4o-mini)...\")\n",
        "results[\"OpenAI (gpt-4o-mini)\"] = query_openai(test_prompt, model=\"gpt-4o-mini\")\n",
        "\n",
        "print(\"Querying Hugging Face (Zephyr-7b-beta)...\")\n",
        "results[\"Hugging Face (zephyr-7b-beta)\"] = query_huggingface(test_prompt, repo_id=\"HuggingFaceH4/zephyr-7b-beta\")\n",
        "\n",
        "print(\"Querying Groq (Llama 3 8B)...\")\n",
        "results[\"Groq (llama3-8b-8192)\"] = query_groq(test_prompt, model=\"llama3-8b-8192\")\n",
        "\n",
        "print(\"Querying Ollama (llama3) - ensure server is running locally...\")\n",
        "results[\"Ollama (llama3)\"] = query_ollama(test_prompt, model=\"llama3\")\n",
        "\n",
        "print(\"Querying Google Gemini (gemini-1.5-flash-latest)...\") # Updated model name\n",
        "results[\"Google Gemini (gemini-1.5-flash-latest)\"] = query_google_gemini(test_prompt, model_name=\"gemini-1.5-flash-latest\")\n",
        "\n",
        "print(\"Querying Anthropic Claude (claude-3-haiku-20240307)...\") # Using Haiku for typically faster response\n",
        "results[\"Anthropic Claude (Haiku)\"] = query_anthropic_claude(test_prompt, model=\"claude-3-haiku-20240307\")\n",
        "\n",
        "print(\"Querying Mistral AI (mistral-small-latest)...\") # Using small for typically faster response\n",
        "results[\"Mistral AI (mistral-small-latest)\"] = query_mistral(test_prompt, model=\"mistral-small-latest\")\n",
        "\n",
        "print(\"--- Comparative Queries Complete ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuFJrmo6NGaq"
      },
      "source": [
        "### Displaying Results\n",
        "\n",
        "We'll organize the collected data into a Pandas DataFrame for easier viewing and comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eQYfRArBNGaq",
        "outputId": "29b7ac17-0146-4848-ac39-8412f5a20211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Summary of Results ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                          Latency  \\\n",
              "Platform                                            \n",
              "OpenAI (gpt-4o-mini)                     4.8944 s   \n",
              "Hugging Face (zephyr-7b-beta)            2.9884 s   \n",
              "Groq (llama3-8b-8192)                    6.0363 s   \n",
              "Ollama (llama3)                               N/A   \n",
              "Google Gemini (gemini-1.5-flash-latest)  3.3684 s   \n",
              "Anthropic Claude (Haiku)                      N/A   \n",
              "Mistral AI (mistral-small-latest)        1.5995 s   \n",
              "\n",
              "                                                                               Tokens Info  \\\n",
              "Platform                                                                                     \n",
              "OpenAI (gpt-4o-mini)                                          In: 33, Out: 150, Total: 183   \n",
              "Hugging Face (zephyr-7b-beta)            Token counts not directly available via simple...   \n",
              "Groq (llama3-8b-8192)                                         In: 37, Out: 150, Total: 187   \n",
              "Ollama (llama3)                                N/A - Token info not available due to error   \n",
              "Google Gemini (gemini-1.5-flash-latest)                      In: N/A, Out: 133, Total: 158   \n",
              "Anthropic Claude (Haiku)                       N/A - Token info not available due to error   \n",
              "Mistral AI (mistral-small-latest)                             In: 30, Out: 150, Total: 180   \n",
              "\n",
              "                                                                          Response Excerpt  \n",
              "Platform                                                                                    \n",
              "OpenAI (gpt-4o-mini)                     Artificial General Intelligence (AGI) refers t...  \n",
              "Hugging Face (zephyr-7b-beta)             Use clear and concise language, and provide e...  \n",
              "Groq (llama3-8b-8192)                    Artificial General Intelligence (AGI) refers t...  \n",
              "Ollama (llama3)                          Error querying Ollama: Failed to connect to Ol...  \n",
              "Google Gemini (gemini-1.5-flash-latest)  Artificial General Intelligence (AGI) refers t...  \n",
              "Anthropic Claude (Haiku)                 Error querying Anthropic Claude: Error code: 4...  \n",
              "Mistral AI (mistral-small-latest)        Artificial General Intelligence (AGI) refers t...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a7b85db3-746a-4042-a829-bb5a5f3be2a9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Latency</th>\n",
              "      <th>Tokens Info</th>\n",
              "      <th>Response Excerpt</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Platform</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>OpenAI (gpt-4o-mini)</th>\n",
              "      <td>4.8944 s</td>\n",
              "      <td>In: 33, Out: 150, Total: 183</td>\n",
              "      <td>Artificial General Intelligence (AGI) refers t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Hugging Face (zephyr-7b-beta)</th>\n",
              "      <td>2.9884 s</td>\n",
              "      <td>Token counts not directly available via simple...</td>\n",
              "      <td>Use clear and concise language, and provide e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Groq (llama3-8b-8192)</th>\n",
              "      <td>6.0363 s</td>\n",
              "      <td>In: 37, Out: 150, Total: 187</td>\n",
              "      <td>Artificial General Intelligence (AGI) refers t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ollama (llama3)</th>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A - Token info not available due to error</td>\n",
              "      <td>Error querying Ollama: Failed to connect to Ol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Google Gemini (gemini-1.5-flash-latest)</th>\n",
              "      <td>3.3684 s</td>\n",
              "      <td>In: N/A, Out: 133, Total: 158</td>\n",
              "      <td>Artificial General Intelligence (AGI) refers t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Anthropic Claude (Haiku)</th>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A - Token info not available due to error</td>\n",
              "      <td>Error querying Anthropic Claude: Error code: 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mistral AI (mistral-small-latest)</th>\n",
              "      <td>1.5995 s</td>\n",
              "      <td>In: 30, Out: 150, Total: 180</td>\n",
              "      <td>Artificial General Intelligence (AGI) refers t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7b85db3-746a-4042-a829-bb5a5f3be2a9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a7b85db3-746a-4042-a829-bb5a5f3be2a9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a7b85db3-746a-4042-a829-bb5a5f3be2a9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-48d9294d-f749-4c47-a394-1b5327566cac\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-48d9294d-f749-4c47-a394-1b5327566cac')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-48d9294d-f749-4c47-a394-1b5327566cac button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(\\\"\\\\n\\\" + \\\"-\\\"*80 + \\\"\\\\n\\\")\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"Platform\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"OpenAI (gpt-4o-mini)\",\n          \"Hugging Face (zephyr-7b-beta)\",\n          \"Anthropic Claude (Haiku)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Latency\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"4.8944 s\",\n          \"2.9884 s\",\n          \"1.5995 s\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tokens Info\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"In: 33, Out: 150, Total: 183\",\n          \"Token counts not directly available via simple text_generation, estimate based on length.\",\n          \"In: 30, Out: 150, Total: 180\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Response Excerpt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Artificial General Intelligence (AGI) refers to a type of artificial intelligence that possesses the...\",\n          \" Use clear and concise language, and provide examples to illustrate your explanation. Avoid technica...\",\n          \"Error querying Anthropic Claude: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_requ...\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Full Responses for Qualitative Analysis ---\n",
            "\n",
            "### OpenAI (gpt-4o-mini)\n",
            "\n",
            "Latency: 4.8944 s\n",
            "Tokens Info: In: 33, Out: 150, Total: 183\n",
            "Response:\n",
            "Artificial General Intelligence (AGI) refers to a type of artificial intelligence that possesses the ability to understand, learn, and apply knowledge across a wide range of tasks at a level comparable to that of a human being. Unlike narrow AI, which excels in specific tasks, AGI would be capable of reasoning, problem-solving, and adapting to new situations with a degree of flexibility and creativity. The potential impact of AGI on society and daily life could be profound, leading to unprecedented advancements in fields such as healthcare, education, and transportation, while also raising ethical concerns about job displacement, autonomy, and decision-making. As AGI systems could enhance productivity and innovation, their integration into society could transform how we work, interact, and address complex global challenges,\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "### Hugging Face (zephyr-7b-beta)\n",
            "\n",
            "Latency: 2.9884 s\n",
            "Tokens Info: Token counts not directly available via simple text_generation, estimate based on length.\n",
            "Response:\n",
            " Use clear and concise language, and provide examples to illustrate your explanation. Avoid technical jargon or overly complex explanations. Aim to make your paragraph accessible to a general audience with little prior knowledge in the field of AI.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "### Groq (llama3-8b-8192)\n",
            "\n",
            "Latency: 6.0363 s\n",
            "Tokens Info: In: 37, Out: 150, Total: 187\n",
            "Response:\n",
            "Artificial General Intelligence (AGI) refers to a hypothetical intelligence that possesses the ability to understand, learn, and apply knowledge across a wide range of tasks, much like human intelligence. If achieved, AGI would have a profound impact on society and daily life, potentially transforming the way we live, work, and interact with each other. With AGI, machines would be able to learn, reason, and apply knowledge in a way that is indistinguishable from human intelligence, enabling them to perform any intellectual task that a human can, such as creative problem-solving, decision-making, and critical thinking. This could lead to significant advancements in fields like healthcare, education, finance, and transportation, among others. However, it also raises concerns about job\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "### Ollama (llama3)\n",
            "\n",
            "Latency: N/A\n",
            "Tokens Info: N/A - Token info not available due to error\n",
            "Response:\n",
            "Error querying Ollama: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download. Make sure Ollama server is running and model 'llama3' is pulled (e.g., `ollama pull llama3`).\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "### Google Gemini (gemini-1.5-flash-latest)\n",
            "\n",
            "Latency: 3.3684 s\n",
            "Tokens Info: In: N/A, Out: 133, Total: 158\n",
            "Response:\n",
            "Artificial General Intelligence (AGI) refers to hypothetical artificial intelligence with human-level cognitive abilities, capable of learning, understanding, and applying knowledge across a wide range of tasks without explicit programming for each.  Its potential impact on society is transformative and potentially disruptive, ranging from revolutionizing industries like healthcare and manufacturing through automation and breakthroughs in scientific research, to raising profound ethical questions about job displacement, bias in algorithms, and the very nature of human intelligence and existence.  AGI could fundamentally alter daily life, impacting everything from transportation and communication to education and entertainment, potentially creating unprecedented levels of efficiency and convenience but also posing significant challenges to societal structures and individual well-being.\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "### Anthropic Claude (Haiku)\n",
            "\n",
            "Latency: N/A\n",
            "Tokens Info: N/A - Token info not available due to error\n",
            "Response:\n",
            "Error querying Anthropic Claude: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "### Mistral AI (mistral-small-latest)\n",
            "\n",
            "Latency: 1.5995 s\n",
            "Tokens Info: In: 30, Out: 150, Total: 180\n",
            "Response:\n",
            "Artificial General Intelligence (AGI) refers to hypothetical AI systems that possess the ability to understand, learn, and apply knowledge across a wide range of tasks at a level equal to or beyond human capabilities. Unlike current AI, which excels in narrow tasks, AGI would have the flexibility and adaptability to perform any intellectual task that a human can. The potential impact of AGI on society and daily life is profound. It could revolutionize industries by automating complex decision-making processes, leading to unprecedented efficiency and innovation. However, it also raises significant ethical and societal challenges, including job displacement, privacy concerns, and the need for robust regulatory frameworks. In daily life, AGI could assist in everything from personalized healthcare and education to smart home management\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "comparison_data = []\n",
        "\n",
        "for platform, (response, latency, tokens_info) in results.items():\n",
        "    response_excerpt = response[:100].replace('\\n', ' ') + \"...\" if response and isinstance(response, str) and \"Error:\" not in response else \"N/A - Error or No Response\"\n",
        "    latency_str = f\"{latency:.4f} s\" if latency is not None else \"N/A\"\n",
        "\n",
        "    # Add checks for tokens_info being None before accessing keys\n",
        "    if tokens_info is not None:\n",
        "        input_tokens = tokens_info.get('prompt_tokens', tokens_info.get('input_tokens', 'N/A'))\n",
        "        output_tokens = tokens_info.get('completion_tokens', tokens_info.get('output_tokens', tokens_info.get('candidates_token_count', 'N/A')))\n",
        "        total_tokens = tokens_info.get('total_tokens', tokens_info.get('total_token_count', 'N/A'))\n",
        "\n",
        "        if input_tokens == 'N/A' and 'info' in tokens_info:\n",
        "            tokens_summary = tokens_info['info']\n",
        "        else:\n",
        "            tokens_summary = f\"In: {input_tokens}, Out: {output_tokens}, Total: {total_tokens}\"\n",
        "    else:\n",
        "        tokens_summary = \"N/A - Token info not available due to error\"\n",
        "        input_tokens = 'N/A'\n",
        "        output_tokens = 'N/A'\n",
        "        total_tokens = 'N/A'\n",
        "\n",
        "\n",
        "    comparison_data.append({\n",
        "        \"Platform\": platform,\n",
        "        \"Latency\": latency_str,\n",
        "        \"Input Tokens\": input_tokens,\n",
        "        \"Output Tokens\": output_tokens,\n",
        "        \"Total Tokens\": total_tokens,\n",
        "        \"Tokens Info\": tokens_summary,\n",
        "        \"Response Excerpt\": response_excerpt,\n",
        "        \"Full Response\": response if response and isinstance(response, str) and \"Error:\" not in response else f\"Error: {response}\"\n",
        "    })\n",
        "\n",
        "df_comparison = pd.DataFrame(comparison_data)\n",
        "df_comparison = df_comparison.set_index(\"Platform\")\n",
        "\n",
        "print(\"\\n--- Summary of Results ---\")\n",
        "display(df_comparison[['Latency', 'Tokens Info', 'Response Excerpt']])\n",
        "\n",
        "print(\"\\n--- Full Responses for Qualitative Analysis ---\")\n",
        "for index, row in df_comparison.iterrows():\n",
        "    print(f\"\\n### {index}\\n\")\n",
        "    print(f\"Latency: {row['Latency']}\")\n",
        "    print(f\"Tokens Info: {row['Tokens Info']}\")\n",
        "    print(f\"Response:\\n{row['Full Response']}\")\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iSLtVfbNGaq"
      },
      "source": [
        "## 4. Qualitative Analysis\n",
        "\n",
        "Beyond quantitative metrics like latency and token count, the *quality* of the generated response is paramount. This section requires your manual review and critical assessment.\n",
        "\n",
        "**Review the \"Full Responses\" printed above for each platform and consider the following:**\n",
        "\n",
        "* **Accuracy**: Is the explanation of AGI correct and factual?\n",
        "* **Completeness**: Does it cover the key aspects mentioned in the prompt (concept, potential impact)?\n",
        "* **Coherence & Readability**: Is the language natural, easy to understand, and well-structured?\n",
        "* **Conciseness**: Does it adhere to the \"one paragraph\" constraint effectively?\n",
        "* **Nuance**: Does it acknowledge both positive and negative potential impacts?\n",
        "* **Bias**: Does the response exhibit any noticeable biases?\n",
        "\n",
        "**Your Observations (add your notes here):**\n",
        "\n",
        "**OpenAI (gpt-3.5-turbo)**:\n",
        "-\n",
        "\n",
        "**Hugging Face (zephyr-7b-beta)**:\n",
        "-\n",
        "\n",
        "**Groq (llama3-8b-8192)**:\n",
        "-\n",
        "\n",
        "**Ollama (llama3)**:\n",
        "-\n",
        "\n",
        "**Google Gemini (gemini-pro)**:\n",
        "-\n",
        "\n",
        "**Anthropic Claude (Haiku)**:\n",
        "-\n",
        "\n",
        "**Mistral AI (mistral-small-latest)**:\n",
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQaBgol5NGaq"
      },
      "source": [
        "## 5. Cost Analysis\n",
        "\n",
        "Cost is a critical factor for production applications. While we have retrieved token counts, you will need to refer to each platform's official pricing documentation to calculate the actual cost based on your anticipated usage.\n",
        "\n",
        "**General Pricing Model**: Most commercial LLM APIs charge per token, often with different rates for input (prompt) tokens and output (completion) tokens. Prices are typically quoted per 1,000 or 1,000,000 tokens.\n",
        "\n",
        "**Key Steps for Cost Estimation**:\n",
        "\n",
        "1.  **Retrieve Token Counts**: Use the `Input Tokens` and `Output Tokens` from our results.\n",
        "2.  **Check Current Pricing**: Visit the official pricing pages for each platform.\n",
        "    * **OpenAI**: [openai.com/pricing](https://openai.com/pricing)\n",
        "    * **Hugging Face Inference API**: Pricing depends on the underlying provider; check the specific model's page or Hugging Face paid Inference Endpoints.\n",
        "    * **Groq**: [groq.com/pricing](https://groq.com/pricing)\n",
        "    * **Ollama**: Free to use locally (costs are your hardware and electricity).\n",
        "    * **Google Gemini**: [ai.google.dev/gemini-api/docs/pricing](https://ai.google.dev/gemini-api/docs/pricing)\n",
        "    * **Anthropic**: [anthropic.com/pricing](https://anthropic.com/pricing)\n",
        "    * **Mistral AI**: [mistral.ai/pricing](https://mistral.ai/pricing)\n",
        "3.  **Calculate Estimated Cost**: `(Input Tokens / 1,000,000) * Input Price Per Million + (Output Tokens / 1,000,000) * Output Price Per Million`\n",
        "\n",
        "**Example Cost Calculation (Hypothetical for Illustration):**\n",
        "\n",
        "If OpenAI GPT-3.5-turbo costs $0.50 per 1M input tokens and $1.50 per 1M output tokens, and your query used 50 input tokens and 100 output tokens:\n",
        "\n",
        "Input Cost = $(50 / 1,000,000) * 0.50 = $0.000025\n",
        "Output Cost = $(100 / 1,000,000) * 1.50 = $0.00015\n",
        "Total Cost = $0.000175 per query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2vPAn_iNGar"
      },
      "source": [
        "## 6. Conclusion and Next Steps\n",
        "\n",
        "This comparative analysis provides a snapshot of different LLM platforms based on a single prompt. Your findings will likely vary based on the specific models chosen, prompt complexity, `max_tokens`, and `temperature` settings.\n",
        "\n",
        "**Summary of Findings (Your interpretation):**\n",
        "\n",
        "* Which platform offered the lowest latency for this task?\n",
        "* Which platform offered the most comprehensive token usage reporting?\n",
        "* Which platform provided the highest quality response according to your qualitative assessment?\n",
        "* What are the perceived trade-offs between speed, quality, and cost for these platforms?\n",
        "\n",
        "**Further Analysis Ideas:**\n",
        "\n",
        "* **Vary Prompt Length/Complexity**: Test with very short and very long prompts.\n",
        "* **Specific Tasks**: Run benchmarks for tasks critical to your application (e.g., summarization, code generation, sentiment analysis).\n",
        "* **Error Handling**: Implement more robust error handling and retry mechanisms.\n",
        "* **Streaming Responses**: Evaluate time to first token (TTFT) for streaming APIs, which is crucial for real-time user experiences.\n",
        "* **Concurrency**: Test how each API performs under concurrent requests.\n",
        "* **Cost Optimization**: Explore strategies like prompt compression, caching, and choosing smaller models where appropriate."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}